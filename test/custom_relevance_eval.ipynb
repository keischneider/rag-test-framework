{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb74c8f9",
   "metadata": {},
   "source": [
    "# Plan ðŸ§­\n",
    "##### Choose the most critical metric for RAG for my use case (banking chatbot)\n",
    "1. relevancy\n",
    "2. groundedness\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884a4d46",
   "metadata": {},
   "source": [
    "## Relevancy evaluation\n",
    "\n",
    "#### Questions\n",
    "- what is relevancy in LLM context?\n",
    "- how to measure it?\n",
    "- what are the common approaches?\n",
    "- what are the common pitfalls?\n",
    "- what are the best practices?\n",
    "- how to implement it?\n",
    "- how to validate it?\n",
    "- how to benchmark it?\n",
    "- how to improve it?\n",
    "\n",
    "#### 1. Embedding based relevancy evaluator \n",
    "\n",
    "- To check relevancy, we can use cosine similarity between embeddings of the question and the answer.\n",
    "- We can use OpenAI's embedding model to generate embeddings for both the question and the answer.\n",
    "- Then, we can calculate the cosine similarity between the two embeddings.\n",
    "- If the cosine similarity is above a certain threshold, we can consider the answer to be relevant to the question.\n",
    "- We can use the following steps to implement this:\n",
    "    - Generate embeddings for the question and the answer using OpenAI's embedding model\n",
    "    - Calculate the cosine similarity between the two embeddings\n",
    "#### 2. LLM-as-a-judge evaluator\n",
    "\n",
    "- We can use a language model to evaluate the relevancy of the answer to the question\n",
    "- by prompting the model with the question and the answer and asking it to rate the relevancy on a scale of 1 to 10.\n",
    "- We can use the following steps to implement this:\n",
    "    - Create a prompt that includes the question and the answer\n",
    "    - Use the language model to generate a response to the prompt\n",
    "    - Parse the response to extract the relevancy rating\n",
    "\n",
    "## Sources ðŸ“š\n",
    "### Microsoft\n",
    "- https://github.com/Azure/azure-sdk-for-python/tree/azure-ai-evaluation_1.0.0b5/sdk/evaluation/azure-ai-evaluation\n",
    "- https://github.com/Azure/azure-sdk-for-python/blob/azure-ai-evaluation_1.0.0b5/sdk/evaluation/azure-ai-evaluation/azure/ai/evaluation/_evaluators/_relevance/relevance.prompty\n",
    "- Azure AI search tailored relevance evaluator - https://github.com/Azure-Samples/azureai-samples/blob/main/scenarios/evaluate/Supported_Evaluation_Metrics/RAG_Evaluation/Optimize_RAG_with_Document_Retrieval_Evaluator.ipynb\n",
    "\n",
    "### Ragas\n",
    "- https://docs.ragas.io/en/stable/getstarted/rag_eval/#load-documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b541c204",
   "metadata": {},
   "source": [
    "## Groundedness\n",
    "\n",
    "#### 1. Entity-based groundedness evaluator\n",
    "- To check groundedness, we can extract entities from the answer and check if they are present in the context.\n",
    "- We can use a named entity recognition (NER) model to extract entities from the answer.\n",
    "- Then, we can check if the extracted entities are present in the context.\n",
    "- If a certain percentage of the extracted entities are present in the context, we can consider the answer to be grounded.\n",
    "- We can use the following steps to implement this:\n",
    "    - Use a NER model to extract entities from the answer\n",
    "    - Check if the extracted entities are present in the context\n",
    "#### 2. LLM-as-a-judge evaluator\n",
    "- We can use a language model to evaluate the groundedness of the answer by prompting the model with the context and the answer\n",
    "- and asking it to rate the groundedness on a scale of 1 to 10.\n",
    "- We can use the following steps to implement this:\n",
    "    - Create a prompt that includes the context and the answer\n",
    "    - Use the language model to generate a response to the prompt\n",
    "    - Parse the response to extract the groundedness rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d72f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Relevance Evaluation\n",
    "# use openai embedding model to get embeddings for query and output\n",
    "# then compute cosine similarity between them\n",
    "# if similarity > threshold, then output is relevant\n",
    "# then ask LLM to rate the relevance of the output on a scale of 0-1 and provide detailed explanation of the score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31482d49",
   "metadata": {},
   "source": [
    "### source: https://cookbook.openai.com/examples/question_answering_using_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4fe59ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from test_data.utils import save_to_file\n",
    "import time\n",
    "import json\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134c194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are a text analytics, who evaluates relevance of an answer to a given question.\n",
    "You will be given a question and an answer, the goal is to deeply analyse it and come up with a score from 0 to 1, where 0 means the answer is completely irrelevant, and 1 - is the answer is absolutely relevant. Always conclude your evaluation with detailed but concise explanation of your scoring, supported by concrete examples and other arguments.\n",
    "The example format for your answer is a JSONL file with the following structure:\n",
    "\"input\": \"<provided question>\", \"output\": \"<provided answer>\", \"score\": \"<your score>\", \"score_explanation\": \"<your score explanation>\"\\\n",
    "\n",
    "Example:\n",
    "Question: I see a charge on my credit card for $2,500 to \"ElectroMax\" on 2025-08-01 that I don't recognize. The card is 4111 1111 1111 1111. Please investigate and take action. \n",
    "Answer: Immediate response (sensitive action): 1) Confirm identity: ask customer to verify full name, date of birth (30), and last 4 of account (6789) or last 4 of card (1111). 2) While verifying, place an immediate temporary block on card number 4111 1111 1111 1111 to prevent further charges. 3) Create a fraud case with severity: high; record: merchant \"ElectroMax\", amount $2,500, date 2025-08-01, transaction ID (if available). 4) Provisionally credit the $2,500 pending investigation (explain provisional credit policy and timeline). 5) Start dispute process: request client-supplied evidence (receipt, photos, merchant communication), and confirm whether customer wants a replacement card issued. 6) Notify customer via their preferred contact_method (email: john.doe@example.com) and inform that two-factor authentication will be required for any account changes. 7) Next steps for customer: respond with a copy/photo of the cardholderâ€™s ID and a short written statement if prompted, confirm whether they made any recent authorized transactions or gave card to a family member. 8) Internal actions: escalate to fraud team, monitor account 123456789 and routing 987654321 for suspicious transfers, log event with timestamp, agent ID, and actions taken. Estimated resolution time: 7â€“10 business days for full investigation; provisional credit may take 3â€“5 business days. If urgent, provide phone support option to +1-202-555-0199.\n",
    "\n",
    "Your response in JSONL format:\n",
    "\"input\": \"I see a charge on my credit card for $2,500 to \"ElectroMax\" on 2025-08-01 that I don't recognize. The card is 4111 1111 1111 1111. Please investigate and take action.\", \"output\": \"Immediate response (sensitive action): 1) Confirm identity: ask customer to verify full name, date of birth (30), and last 4 of account (6789) or last 4 of card (1111). 2) While verifying, place an immediate temporary block on card number 4111 1111 1111 1111 to prevent further charges. 3) Create a fraud case with severity: high; record: merchant \"ElectroMax\", amount $2,500, date 2025-08-01, transaction ID (if available). 4) Provisionally credit the $2,500 pending investigation (explain provisional credit policy and timeline). 5) Start dispute process: request client-supplied evidence (receipt, photos, merchant communication), and confirm whether customer wants a replacement card issued. 6) Notify customer via their preferred contact_method (email: john.doe@example.com) and inform that two-factor authentication will be required for any account changes. 7) Next steps for customer: respond with a copy/photo of the cardholderâ€™s ID and a short written statement if prompted, confirm whether they made any recent authorized transactions or gave card to a family member. 8) Internal actions: escalate to fraud team, monitor account 123456789 and routing 987654321 for suspicious transfers, log event with timestamp, agent ID, and actions taken. Estimated resolution time: 7â€“10 business days for full investigation; provisional credit may take 3â€“5 business days. If urgent, provide phone support option to +1-202-555-0199.\", \"score\": \"0.798\", \"score_explanation\": \"The answer is relevant, but there is some redundant information and some missing points. For example this part 'Immediate response (sensitive action)' is confusing. Another point is redandancy of steps, instead 8, there could be 5. Strengths and weaknesses: ... . The answer not totally answers the customer question about charing recognition.\"\n",
    "\n",
    "Here is the question and answer to evaluate:\n",
    "Question: {question}\n",
    "Answer: {answer}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "45a76676",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "test_questions_path = \"../test_data/test_questions/344342342.jsonl\"\n",
    "df = pd.read_json(test_questions_path, lines=True)\n",
    "relatedness_fn = lambda x, y: 1 - spatial.distance.cosine(x, y)\n",
    "\n",
    "query_embedding = client.embeddings.create(\n",
    "    model='text-embedding-3-small',\n",
    "    input=df.iloc[0]['input'],\n",
    ")\n",
    "response_embedding = client.embeddings.create(\n",
    "    model='text-embedding-3-small',\n",
    "    input=df.iloc[0]['output'],\n",
    ")\n",
    "cosine_similarity = relatedness_fn(\n",
    "    query_embedding.data[0].embedding,\n",
    "    response_embedding.data[0].embedding\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': prompt.format(question=df.iloc[0]['input'],answer=df.iloc[0]['output'])}\n",
    "    ],\n",
    "    model='gpt-5-mini'\n",
    ")\n",
    "response_text = response.choices[0].message.content\n",
    "data = json.loads(response_text)\n",
    "data['cosine_similarity'] = cosine_similarity\n",
    "df = pd.read_json(io.StringIO(json.dumps(data)), lines=True)\n",
    "df.to_csv(f'{int(time.time())}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
